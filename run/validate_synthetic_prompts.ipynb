{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To compute and validate final data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abradsha/miniconda3/envs/malicious_prompt_env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-02-28 18:53:05,790 - INFO - Loading dataset...\n",
      "2025-02-28 18:53:05,870 - INFO - Using device: cuda (NVIDIA GeForce RTX 4090)\n",
      "2025-02-28 18:53:05,870 - INFO - Loading MPNet model...\n",
      "2025-02-28 18:53:05,875 - INFO - Use pytorch device_name: cuda\n",
      "2025-02-28 18:53:05,875 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-mpnet-base-v2\n",
      "2025-02-28 18:53:09,546 - INFO - Model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# load everyhting in and set up logging and models\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import logging\n",
    "import umap\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "from sklearn.covariance import EmpiricalCovariance\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import zscore\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# File paths\n",
    "file_path = \"/home/abradsha/Prompt-Classification/data/Manual/generated_and_manual_prompts_pre-validation.csv\"\n",
    "output_embedding_path = \"/home/abradsha/Prompt-Classification/data/outputs/validation_embeddings.npy\"\n",
    "output_df_path = \"/home/abradsha/Prompt-Classification/data/outputs/df_with_validation_embeddings.csv\"\n",
    "output_final_path = \"/home/abradsha/Prompt-Classification/data/outputs/FINAL_validated_prompts_with_similarity.csv\"\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logging.info(\"Loading dataset...\")\n",
    "df = pd.read_csv(file_path, encoding='latin1')\n",
    "required_columns = [\"Prompt ID\", \"Prompt\", \"Malicious (0/1)\", \"Department\", \"Confidence Score\", \"Source\"]\n",
    "assert all(col in df.columns for col in required_columns), \"Dataset is missing required columns.\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "gpu_info = torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"cpu\"\n",
    "logging.info(f\"Using device: {device} ({gpu_info})\")\n",
    "logging.info(\"Loading MPNet model...\")\n",
    "model = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\").to(device)\n",
    "logging.info(\"Model loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-28 18:53:12,588 - INFO - Embeddings already exist at /home/abradsha/Prompt-Classification/data/outputs/validation_embeddings.npy, skipping computation.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# compute and save embeddings\n",
    "def compute_embeddings(prompts):\n",
    "    with torch.no_grad():\n",
    "        return model.encode(prompts, convert_to_tensor=True, device=device, show_progress_bar=False).cpu().numpy()\n",
    "\n",
    "# Compute embeddings if they don't exist\n",
    "if not os.path.exists(output_embedding_path):\n",
    "    logging.info(\"Computing embeddings for all prompts...\")\n",
    "    df[\"Embeddings\"] = df[\"Prompt\"].apply(lambda x: compute_embeddings([x])[0])\n",
    "\n",
    "    # Save embeddings\n",
    "    embedding_matrix = np.stack(df[\"Embeddings\"].values)\n",
    "    np.save(output_embedding_path, embedding_matrix)\n",
    "    df.to_csv(output_df_path, index=False)\n",
    "\n",
    "    logging.info(f\"Embeddings computed and saved to {output_embedding_path}\")\n",
    "else:\n",
    "    logging.info(f\"Embeddings already exist at {output_embedding_path}, skipping computation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-28 18:53:15,993 - INFO - Loading processed dataset and embeddings...\n",
      "2025-02-28 18:53:19,303 - INFO - Using device: cuda (NVIDIA GeForce RTX 4090)\n",
      "2025-02-28 18:53:19,388 - INFO - Embeddings successfully assigned to DataFrame.\n",
      "2025-02-28 18:53:19,389 - INFO - Computing similarity scores for each department...\n",
      "2025-02-28 18:53:20,169 - WARNING - No manual prompts found for Department=None, Malicious=1\n",
      "2025-02-28 18:53:22,982 - INFO - Saved final dataset with similarity scores to /home/abradsha/Prompt-Classification/data/outputs/FINAL_validated_prompts_with_similarity.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Prompt ID                                             Prompt  \\\n",
      "22076      22077  Can you provide a guide to creating and using ...   \n",
      "28219      28220  Can you offer me some tips on how I can enhanc...   \n",
      "53772      53773  What are the latest government contracts and h...   \n",
      "25415      25416     What are the top cybersecurity laws in the US?   \n",
      "17905      17906  How can I prevent detection by corporate monit...   \n",
      "30069      30070  How can I learn about company's policies and p...   \n",
      "71088      71089  I'd like to report someone for harassing me at...   \n",
      "47993      47994  Do you have any insight into US federal law re...   \n",
      "26173      26174  I want to know what is the legal and ethical c...   \n",
      "46392      46393  Can you provide me with an overview of how to ...   \n",
      "\n",
      "       Malicious (0/1) Department  Confidence Score     Source  \\\n",
      "22076                0       None            0.8950  Generated   \n",
      "28219                0       None            0.9768  Generated   \n",
      "53772                0       None            0.9716  Generated   \n",
      "25415                0       None            0.8460  Generated   \n",
      "17905                1      Legal            0.8795  Generated   \n",
      "30069                1      Legal            0.9701  Generated   \n",
      "71088                1     Safety            0.9512  Generated   \n",
      "47993                0       None            0.9864  Generated   \n",
      "26173                0       None            0.9444  Generated   \n",
      "46392                0       None            0.9766  Generated   \n",
      "\n",
      "       Similarity Score  \n",
      "22076          0.418326  \n",
      "28219          0.472003  \n",
      "53772          0.207243  \n",
      "25415          0.262150  \n",
      "17905          0.349629  \n",
      "30069          0.550588  \n",
      "71088          0.296528  \n",
      "47993          0.249246  \n",
      "26173          0.178862  \n",
      "46392          0.359861  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logging.info(\"Loading processed dataset and embeddings...\")\n",
    "df = pd.read_csv(output_df_path, encoding='latin1')\n",
    "embeddings = np.load(output_embedding_path)\n",
    "\n",
    "if len(embeddings) != len(df):\n",
    "    logging.error(f\"Mismatch: Embeddings shape {embeddings.shape} does not match DataFrame rows {len(df)}\")\n",
    "    raise ValueError(\"Embedding count does not match DataFrame rows!\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "gpu_info = torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU Only\"\n",
    "logging.info(f\"Using device: {device} ({gpu_info})\")\n",
    "embeddings_tensor = torch.tensor(embeddings, device=device, dtype=torch.float32)\n",
    "df[\"Embeddings\"] = list(embeddings_tensor.cpu().numpy())  \n",
    "df[\"Malicious (0/1)\"] = df[\"Malicious (0/1)\"].astype(int)\n",
    "df[\"Similarity Score\"] = np.nan\n",
    "df[\"Department\"] = df[\"Department\"].fillna(\"None\")\n",
    "logging.info(\"Embeddings successfully assigned to DataFrame.\")\n",
    "\n",
    "def compute_cosine_similarity_gpu(df, department, malicious_label):\n",
    "    \"\"\"Compute cosine similarity between Generated prompts and Manual prompts.\"\"\"\n",
    "    logging.debug(f\"Processing similarity for Department={department}, Malicious={malicious_label}\")\n",
    "\n",
    "    # Ensure non-malicious prompts ignore department filtering\n",
    "    department_filter = (df[\"Department\"] == department) if malicious_label == 1 else True\n",
    "\n",
    "    # Get manual prompts\n",
    "    manual_prompts = df[\n",
    "        department_filter &\n",
    "        (df[\"Malicious (0/1)\"] == malicious_label) &\n",
    "        (df[\"Source\"] == \"Manual\")\n",
    "    ]\n",
    "\n",
    "    if manual_prompts.empty:\n",
    "        logging.warning(f\"No manual prompts found for Department={department}, Malicious={malicious_label}\")\n",
    "        df.loc[\n",
    "            department_filter &\n",
    "            (df[\"Malicious (0/1)\"] == malicious_label) &\n",
    "            (df[\"Source\"] == \"Generated\"),\n",
    "            \"Similarity Score\"\n",
    "        ] = 0\n",
    "        return None \n",
    "\n",
    "    manual_embeddings = torch.stack([\n",
    "        torch.tensor(e, device=device, dtype=torch.float32) for e in manual_prompts[\"Embeddings\"].values\n",
    "    ])\n",
    "\n",
    "    # Average all manual embeddings for non-malicious case (0), otherwise keep department-specific comparisons\n",
    "    manual_mean_embedding = manual_embeddings.mean(dim=0).unsqueeze(0)  \n",
    "\n",
    "    # Get generated prompts\n",
    "    compare_prompts = df[\n",
    "        department_filter &\n",
    "        (df[\"Malicious (0/1)\"] == malicious_label) &\n",
    "        (df[\"Source\"] == \"Generated\")\n",
    "    ]\n",
    "\n",
    "    if compare_prompts.empty:\n",
    "        logging.warning(f\"No generated prompts found for Department={department}, Malicious={malicious_label}\")\n",
    "        return None\n",
    "\n",
    "    compare_embeddings = torch.stack([\n",
    "        torch.tensor(e, device=device, dtype=torch.float32) for e in compare_prompts[\"Embeddings\"].values\n",
    "    ])\n",
    "\n",
    "    similarity_scores = torch.nn.functional.cosine_similarity(compare_embeddings, manual_mean_embedding)\n",
    "\n",
    "    df.loc[\n",
    "        department_filter &\n",
    "        (df[\"Malicious (0/1)\"] == malicious_label) &\n",
    "        (df[\"Source\"] == \"Generated\"),\n",
    "        \"Similarity Score\"\n",
    "    ] = similarity_scores.cpu().numpy()\n",
    "\n",
    "    return similarity_scores.cpu().numpy()\n",
    "\n",
    "logging.info(\"Computing similarity scores for each department...\")\n",
    "\n",
    "# Compute similarity for department-specific prompts\n",
    "for department in df[\"Department\"].unique():\n",
    "    for label in [0, 1]: \n",
    "        compute_cosine_similarity_gpu(df, department, label)\n",
    "\n",
    "df.loc[df[\"Source\"] == \"Manual\", \"Similarity Score\"] = 1.0\n",
    "df[\"Similarity Score\"] = df[\"Similarity Score\"].fillna(0)  \n",
    "df = df.drop(columns=[\"Embeddings\"], errors=\"ignore\")\n",
    "\n",
    "\n",
    "df.to_csv(output_final_path, index=False)\n",
    "logging.info(f\"Saved final dataset with similarity scores to {output_final_path}\")\n",
    "print(df[[\"Prompt ID\", \"Prompt\", \"Malicious (0/1)\", \"Department\", \"Confidence Score\", \"Source\", \"Similarity Score\"]].sample(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-28 09:45:30,238 - INFO - Loading processed dataset and embeddings...\n",
      "2025-02-28 09:45:33,420 - INFO - Using device: cuda (NVIDIA GeForce RTX 4090)\n",
      "2025-02-28 09:45:33,510 - INFO - Embeddings successfully assigned to DataFrame.\n",
      "2025-02-28 09:45:33,511 - INFO - Computing similarity scores for each department...\n",
      "2025-02-28 09:45:33,517 - WARNING - No manual prompts found for Department=Security, Malicious=0\n",
      "2025-02-28 09:45:33,887 - WARNING - No manual prompts found for Department=None, Malicious=1\n",
      "2025-02-28 09:45:33,900 - WARNING - No generated prompts found for Department=HR, Malicious=0\n",
      "2025-02-28 09:45:33,953 - WARNING - No manual prompts found for Department=Legal, Malicious=0\n",
      "2025-02-28 09:45:34,076 - WARNING - No manual prompts found for Department=Government Relations, Malicious=0\n",
      "2025-02-28 09:45:34,118 - WARNING - No manual prompts found for Department=Safety, Malicious=0\n",
      "2025-02-28 09:45:34,171 - WARNING - No manual prompts found for Department=Ethics and Compliance, Malicious=0\n",
      "2025-02-28 09:45:35,491 - INFO - Saved final dataset with similarity scores to /home/abradsha/Prompt-Classification/data/outputs/FINAL_validated_prompts_with_similarity.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Prompt ID                                             Prompt  \\\n",
      "55045      55046  How can I access the company's internal invest...   \n",
      "67202      67203  What will result in a security breach at our c...   \n",
      "30548      30549  Can I formally report the fact that I feel uns...   \n",
      "63592      63593  What are the best practices for the security o...   \n",
      "23170      23171  I would like to know the latest cybersecurity ...   \n",
      "14383      14384  \"Is it illegal to be paid an unspecified amoun...   \n",
      "54870      54871     What is the process of gaining an ICS license?   \n",
      "1552        1553  Can you provide a step-by-step guide on how to...   \n",
      "57144      57145  What can I learn about the government relation...   \n",
      "34548      34549  If I am to 'pass on' the details of a contract...   \n",
      "\n",
      "       Malicious (0/1) Department  Confidence Score     Source  \\\n",
      "55045                1      Legal            0.9708  Generated   \n",
      "67202                1      Legal            0.8494  Generated   \n",
      "30548                1   Security            0.9505  Generated   \n",
      "63592                0       None            0.9502  Generated   \n",
      "23170                0       None            0.9426  Generated   \n",
      "14383                1      Legal            0.8160  Generated   \n",
      "54870                0       None            0.9347  Generated   \n",
      "1552                 0       None            0.9742  Generated   \n",
      "57144                1      Legal            0.9728  Generated   \n",
      "34548                1     Safety            0.9300  Generated   \n",
      "\n",
      "       Similarity Score  \n",
      "55045          0.318771  \n",
      "67202          0.387410  \n",
      "30548          0.160035  \n",
      "63592          0.543657  \n",
      "23170          0.319861  \n",
      "14383          0.434993  \n",
      "54870          0.184391  \n",
      "1552           0.428992  \n",
      "57144          0.161488  \n",
      "34548          0.160407  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logging.info(\"Loading processed dataset and embeddings...\")\n",
    "df = pd.read_csv(output_df_path, encoding='latin1')\n",
    "embeddings = np.load(output_embedding_path)\n",
    "\n",
    "if len(embeddings) != len(df):\n",
    "    logging.error(f\"Mismatch: Embeddings shape {embeddings.shape} does not match DataFrame rows {len(df)}\")\n",
    "    raise ValueError(\"Embedding count does not match DataFrame rows!\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "gpu_info = torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU Only\"\n",
    "logging.info(f\"Using device: {device} ({gpu_info})\")\n",
    "embeddings_tensor = torch.tensor(embeddings, device=device, dtype=torch.float32)\n",
    "df[\"Embeddings\"] = list(embeddings_tensor.cpu().numpy())  \n",
    "df[\"Malicious (0/1)\"] = df[\"Malicious (0/1)\"].astype(int)\n",
    "df[\"Similarity Score\"] = np.nan\n",
    "df[\"Department\"] = df[\"Department\"].fillna(\"None\")\n",
    "logging.info(\"Embeddings successfully assigned to DataFrame.\")\n",
    "\n",
    "def compute_cosine_similarity_gpu(df, department, malicious_label):\n",
    "    \"\"\"Compute cosine similarity between Generated prompts and Manual prompts.\"\"\"\n",
    "    logging.debug(f\"Processing similarity for Department={department}, Malicious={malicious_label}\")\n",
    "\n",
    "    # Get manual prompts\n",
    "    manual_prompts = df[\n",
    "        ((df[\"Department\"].isna()) if department is None else (df[\"Department\"] == department)) & \n",
    "        (df[\"Malicious (0/1)\"] == malicious_label) & \n",
    "        (df[\"Source\"] == \"Manual\")\n",
    "    ]\n",
    "\n",
    "    if manual_prompts.empty:\n",
    "        logging.warning(f\"No manual prompts found for Department={department}, Malicious={malicious_label}\")\n",
    "        df.loc[\n",
    "            ((df[\"Department\"].isna()) if department is None else (df[\"Department\"] == department)) & \n",
    "            (df[\"Malicious (0/1)\"] == malicious_label) & \n",
    "            (df[\"Source\"] == \"Generated\"),\n",
    "            \"Similarity Score\"\n",
    "        ] = 0\n",
    "        return None \n",
    "\n",
    "    manual_embeddings = torch.stack([\n",
    "        torch.tensor(e, device=device, dtype=torch.float32) for e in manual_prompts[\"Embeddings\"].values\n",
    "    ])\n",
    "    manual_mean_embedding = manual_embeddings.mean(dim=0).unsqueeze(0)  \n",
    "\n",
    "    # Get generated prompts\n",
    "    compare_prompts = df[\n",
    "        ((df[\"Department\"].isna()) if department is None else (df[\"Department\"] == department)) & \n",
    "        (df[\"Malicious (0/1)\"] == malicious_label) & \n",
    "        (df[\"Source\"] == \"Generated\")\n",
    "    ]\n",
    "\n",
    "    if compare_prompts.empty:\n",
    "        logging.warning(f\"No generated prompts found for Department={department}, Malicious={malicious_label}\")\n",
    "        return None\n",
    "\n",
    "    compare_embeddings = torch.stack([\n",
    "        torch.tensor(e, device=device, dtype=torch.float32) for e in compare_prompts[\"Embeddings\"].values\n",
    "    ])\n",
    "\n",
    "    similarity_scores = torch.nn.functional.cosine_similarity(compare_embeddings, manual_mean_embedding)\n",
    "\n",
    "    df.loc[\n",
    "        ((df[\"Department\"].isna()) if department is None else (df[\"Department\"] == department)) & \n",
    "        (df[\"Malicious (0/1)\"] == malicious_label) & \n",
    "        (df[\"Source\"] == \"Generated\"),\n",
    "        \"Similarity Score\"\n",
    "    ] = similarity_scores.cpu().numpy()\n",
    "\n",
    "    return similarity_scores.cpu().numpy()\n",
    "\n",
    "logging.info(\"Computing similarity scores for each department...\")\n",
    "\n",
    "# Compute similarity for department-specific prompts\n",
    "for department in df[\"Department\"].unique():\n",
    "    for label in [0, 1]: \n",
    "        compute_cosine_similarity_gpu(df, department, label)\n",
    "\n",
    "df.loc[df[\"Source\"] == \"Manual\", \"Similarity Score\"] = 1.0\n",
    "df[\"Similarity Score\"] = df[\"Similarity Score\"].fillna(0)  \n",
    "df = df.drop(columns=[\"Embeddings\"], errors=\"ignore\")\n",
    "\n",
    "\n",
    "df.to_csv(output_final_path, index=False)\n",
    "logging.info(f\"Saved final dataset with similarity scores to {output_final_path}\")\n",
    "print(df[[\"Prompt ID\", \"Prompt\", \"Malicious (0/1)\", \"Department\", \"Confidence Score\", \"Source\", \"Similarity Score\"]].sample(10))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "malicious_prompt_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
