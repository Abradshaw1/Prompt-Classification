{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dcli/miniconda3/envs/malicious_prompt_env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from typing import Tuple, Dict, Any\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datasets import Dataset, concatenate_datasets\n",
    "from sklearn.metrics import (\n",
    "    roc_curve, auc, confusion_matrix, accuracy_score,\n",
    "    precision_recall_fscore_support, precision_score,\n",
    "    recall_score, f1_score, roc_auc_score\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSequenceClassification,\n",
    "    Trainer, TrainingArguments, EarlyStoppingCallback\n",
    ")\n",
    "from peft import get_peft_model, LoraConfig, TaskType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-21 16:39:22,195 - INFO - Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Constants\n",
    "#MODEL_NAME = \"microsoft/deberta-v3-small\"\n",
    "MODEL_NAME = \"./deberta_lora_classifier_full_human2\"\n",
    "MAX_LENGTH = 256\n",
    "BATCH_SIZE = 8\n",
    "NUM_EPOCHS = 100\n",
    "WEIGHT_DECAY = 0.01\n",
    "MODEL_SAVE_PATH = \"./deberta_lora_classifier_full_human2\"\n",
    "ROC_SAVE_PATH1 = \"roc_curve_lora_full_syn2.png\"\n",
    "CM_SAVE_PATH1 = \"confusion_matrix_lora_full_syn2.png\"\n",
    "ROC_SAVE_PATH2 = \"roc_curve_lora_full_human2.png\"\n",
    "CM_SAVE_PATH2 = \"confusion_matrix_lora_full_human2.png\"\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(\"DeBERTa Initial Runs\")\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logger.info(f\"Using device: {device}\")\n",
    "\n",
    "class PromptClassifier:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the classifier with model, tokenizer, and LoRA config.\"\"\"\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            MODEL_NAME, use_fast=False\n",
    "        )\n",
    "        # base_model\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            MODEL_NAME, num_labels=2\n",
    "        ).to(device)\n",
    "        \"\"\"\n",
    "        lora_config = LoraConfig(\n",
    "            task_type=TaskType.SEQ_CLS,\n",
    "            r=8,\n",
    "            lora_alpha=16,\n",
    "            lora_dropout=0.1\n",
    "        )\n",
    "        self.model = get_peft_model(self.base_model, lora_config).to(device)\n",
    "        logger.info(\"DeBERTa LoRA model initialized.\")\n",
    "        \"\"\"\n",
    "    def preprocess_data(self, examples: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Preprocess the input data using the tokenizer.\"\"\"\n",
    "        return self.tokenizer(\n",
    "            examples[\"Prompt\"],\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=MAX_LENGTH\n",
    "        ).to(device)\n",
    "\n",
    "    def prepare_datasets(self, df: pd.DataFrame) -> Tuple[Dataset, Dataset]:\n",
    "        \"\"\"Prepare training and validation datasets.\"\"\"\n",
    "        train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "            df[\"Prompt\"].tolist(),\n",
    "            df[\"Malicious (0/1)\"].tolist(),\n",
    "            test_size=0.2,\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "        train_data = Dataset.from_dict({\n",
    "            \"Prompt\": train_texts,\n",
    "            \"label\": train_labels\n",
    "        })\n",
    "        val_data = Dataset.from_dict({\n",
    "            \"Prompt\": val_texts,\n",
    "            \"label\": val_labels\n",
    "        })\n",
    "\n",
    "        train_data = train_data.map(self.preprocess_data, batched=True)\n",
    "        val_data = val_data.map(self.preprocess_data, batched=True)\n",
    "\n",
    "        return train_data, val_data\n",
    "\n",
    "    def compute_metrics(\n",
    "        self, eval_pred: Tuple[np.ndarray, np.ndarray]\n",
    "    ) -> Dict[str, float]:\n",
    "        \"\"\"Compute evaluation metrics.\"\"\"\n",
    "        logits, labels = eval_pred\n",
    "        predictions = torch.argmax(torch.tensor(logits), dim=-1)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "            labels, predictions, average='binary'\n",
    "        )\n",
    "        acc = accuracy_score(labels, predictions)\n",
    "        return {\n",
    "            \"accuracy\": acc,\n",
    "            \"f1\": f1,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall\n",
    "        }\n",
    "\n",
    "    def train(self, train_data: Dataset, val_data: Dataset) -> None:\n",
    "        \"\"\"Train the model.\"\"\"\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=\"./results\",\n",
    "            eval_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            per_device_train_batch_size=BATCH_SIZE,\n",
    "            per_device_eval_batch_size=BATCH_SIZE,\n",
    "            num_train_epochs=NUM_EPOCHS,\n",
    "            weight_decay=WEIGHT_DECAY,\n",
    "            logging_dir=\"./deberta_logs\",\n",
    "            logging_strategy=\"steps\",\n",
    "            logging_steps=50,\n",
    "            save_total_limit=3,\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"eval_loss\",\n",
    "            greater_is_better=False,\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            report_to=\"none\"\n",
    "        )\n",
    "\n",
    "        early_stopping_callback = EarlyStoppingCallback(\n",
    "            early_stopping_patience=3,\n",
    "            early_stopping_threshold=0.0\n",
    "        )\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_data,\n",
    "            eval_dataset=val_data,\n",
    "            compute_metrics=self.compute_metrics,\n",
    "            callbacks=[early_stopping_callback]\n",
    "        )\n",
    "\n",
    "        trainer.train()\n",
    "        self.save_model(trainer)\n",
    "\n",
    "    def save_model(self, trainer: Trainer) -> None:\n",
    "        \"\"\"Save the trained model and tokenizer.\"\"\"\n",
    "        trainer.save_model(MODEL_SAVE_PATH)\n",
    "        self.tokenizer.save_pretrained(MODEL_SAVE_PATH)\n",
    "        logger.info(f\"Model saved to {MODEL_SAVE_PATH}\")\n",
    "\n",
    "    def evaluate(\n",
    "        self, val_data: Dataset\n",
    "    ) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "        \"\"\"Evaluate the model and return predictions.\"\"\"\n",
    "        test_trainer = Trainer(self.model)\n",
    "        predictions = test_trainer.predict(val_data)\n",
    "\n",
    "        logits = torch.tensor(predictions.predictions)\n",
    "        probs = torch.nn.functional.softmax(logits, dim=-1).numpy()\n",
    "        pred_labels = np.argmax(probs, axis=-1)\n",
    "\n",
    "        return predictions.label_ids, pred_labels, probs[:, 1]\n",
    "\n",
    "    def analyze_misclassifications(\n",
    "        self, val_data: Dataset, true_labels: np.ndarray, \n",
    "        pred_labels: np.ndarray, probs: np.ndarray\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"Analyze misclassified examples.\n",
    "    \n",
    "        Returns:\n",
    "        DataFrame containing misclassified examples with their predictions\n",
    "        and confidence scores.\n",
    "        \"\"\"\n",
    "        # Get original prompts from validation dataset\n",
    "        prompts = val_data['Prompt']\n",
    "    \n",
    "        # Create DataFrame with all predictions\n",
    "        results_df = pd.DataFrame({\n",
    "            'Prompt': prompts,\n",
    "            'True_Label': true_labels,\n",
    "            'Predicted_Label': pred_labels,\n",
    "            'Confidence': probs\n",
    "        })\n",
    "    \n",
    "        # Filter for misclassified examples\n",
    "        misclassified = results_df[results_df['True_Label'] != results_df['Predicted_Label']]\n",
    "    \n",
    "        # Sort by confidence to see most confident mistakes\n",
    "        misclassified = misclassified.sort_values('Confidence', ascending=False)\n",
    "    \n",
    "        logger.info(f\"Total misclassified examples: {len(misclassified)}\")\n",
    "        return misclassified\n",
    "    \n",
    "    def plot_metrics(\n",
    "        self, true_labels: np.ndarray, pred_labels: np.ndarray,\n",
    "        probs: np.ndarray, roc_path: str, cm_path:str\n",
    "    ) -> None:\n",
    "        \"\"\"Plot ROC curve and confusion matrix.\"\"\"\n",
    "        # ROC Curve\n",
    "        fpr, tpr, _ = roc_curve(true_labels, probs)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(\n",
    "            fpr, tpr, color=\"blue\", lw=2,\n",
    "            label=f\"ROC Curve (AUC = {roc_auc:.2f})\"\n",
    "        )\n",
    "        plt.plot([0, 1], [0, 1], color=\"gray\", linestyle=\"--\")\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel(\"False Positive Rate\")\n",
    "        plt.ylabel(\"True Positive Rate\")\n",
    "        plt.title(\"Receiver Operating Characteristic (ROC) Curve\")\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.savefig(\n",
    "            roc_path,\n",
    "            dpi=300,\n",
    "            bbox_inches=\"tight\"\n",
    "        )\n",
    "        plt.close()\n",
    "\n",
    "        # Confusion Matrix\n",
    "        cm = confusion_matrix(true_labels, pred_labels)\n",
    "        plt.figure(figsize=(6, 5))\n",
    "        sns.heatmap(\n",
    "            cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=[\"0\", \"1\"], yticklabels=[\"0\", \"1\"]\n",
    "        )\n",
    "        plt.xlabel(\"Predicted Label\")\n",
    "        plt.ylabel(\"True Label\")\n",
    "        plt.title(\"Confusion Matrix\")\n",
    "        plt.savefig(\n",
    "            cm_path,\n",
    "            dpi=300,\n",
    "            bbox_inches=\"tight\"\n",
    "        )\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|██████████| 61328/61328 [00:04<00:00, 13796.87 examples/s]\n",
      "Map: 100%|██████████| 15332/15332 [00:01<00:00, 12891.41 examples/s]\n",
      "Map: 100%|██████████| 129/129 [00:00<00:00, 8404.37 examples/s]\n",
      "Map: 100%|██████████| 33/33 [00:00<00:00, 5514.20 examples/s]\n",
      "2025-03-21 16:43:32,029 - INFO - Training: 30662 malicious prompts\n",
      "2025-03-21 16:43:32,029 - INFO - Training: 30666 benign prompts\n",
      "2025-03-21 16:43:32,030 - INFO - Validation: 7668 malicious prompts\n",
      "2025-03-21 16:43:32,030 - INFO - Validation: 7664 benign prompts\n",
      "2025-03-21 16:43:32,030 - INFO - Evaluating...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9682\n",
      "Precision: 0.9676\n",
      "Recall: 0.9688\n",
      "F1 Score: 0.9682\n",
      "AUC Score: 0.9955\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "df = pd.read_csv(\n",
    "    \"./data/FINAL_validated_prompts_with_similarity_deBERTa.csv\"\n",
    ")\n",
    "df[\"Malicious (0/1)\"] = df[\"Malicious (0/1)\"].astype(int)\n",
    "human_df = df[df[\"Source\"] == \"Manual\"]\n",
    "final_df = df[df[\"Source\"] == \"Generated\"]\n",
    "\"\"\"\n",
    "# Separate malicious and benign prompts\n",
    "malicious_prompts = df[df[\"Malicious (0/1)\"] == 1]\n",
    "benign_prompts = df[df[\"Malicious (0/1)\"] == 0]\n",
    "\n",
    "# Randomly sample 3000 prompts from each class\n",
    "malicious_sample = malicious_prompts.sample(n=3000, random_state=42)\n",
    "benign_sample = benign_prompts.sample(n=3000, random_state=42)\n",
    "\n",
    "# Combine the samples\n",
    "balanced_df = pd.concat([malicious_sample, benign_sample])\n",
    "# Shuffle the combined dataset\n",
    "balanced_df = balanced_df.sample(frac=1, random_state=42) \\\n",
    "        .reset_index(drop=True)\n",
    "\n",
    "logger.info(f\"Created balanced dataset with {len(balanced_df)} rows\")\n",
    "logger.info(f\"Number of malicious prompts: {sum(balanced_df['Malicious (0/1)'] == 1)}\")\n",
    "logger.info(f\"Number of benign prompts: {sum(balanced_df['Malicious (0/1)'] == 0)}\")\n",
    "\"\"\"\n",
    "# Initialize and train classifier\n",
    "classifier = PromptClassifier()\n",
    "train_data, val_data = classifier.prepare_datasets(final_df)\n",
    "temp1, temp2 = classifier.prepare_datasets(human_df)\n",
    "human_data = concatenate_datasets([temp1, temp2])\n",
    "\n",
    "train_total = len(train_data['label'])\n",
    "val_total = len(val_data['label'])\n",
    "train_1_total = sum(train_data['label'])\n",
    "val_1_total = sum(val_data['label'])\n",
    "logger.info(f\"Training: {train_1_total} malicious prompts\")\n",
    "logger.info(f\"Training: {train_total-train_1_total} benign prompts\")\n",
    "logger.info(f\"Validation: {val_1_total} malicious prompts\")\n",
    "logger.info(f\"Validation: {val_total-val_1_total} benign prompts\")\n",
    "\"\"\"\n",
    "logger.info(\"Starting fine-tuning...\")\n",
    "classifier.train(train_data, val_data)\n",
    "\"\"\"\n",
    "# Evaluate and plot results\n",
    "logger.info(\"Evaluating...\")\n",
    "true_labels, pred_labels, probs = classifier.evaluate(val_data)\n",
    "\n",
    "# Print metrics\n",
    "print(f\"Accuracy: {accuracy_score(true_labels, pred_labels):.4f}\")\n",
    "print(f\"Precision: {precision_score(true_labels, pred_labels):.4f}\")\n",
    "print(f\"Recall: {recall_score(true_labels, pred_labels):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(true_labels, pred_labels):.4f}\")\n",
    "print(f\"AUC Score: {roc_auc_score(true_labels, probs):.4f}\")\n",
    "\n",
    "# Plot metrics\n",
    "classifier.plot_metrics(true_labels, pred_labels, probs, ROC_SAVE_PATH1, CM_SAVE_PATH1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 129/129 [00:00<00:00, 8119.84 examples/s]\n",
      "Map: 100%|██████████| 33/33 [00:00<00:00, 5562.74 examples/s]\n",
      "2025-03-21 16:41:41,649 - INFO - Evaluating on human data...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7222\n",
      "Precision: 0.7222\n",
      "Recall: 1.0000\n",
      "F1 Score: 0.8387\n",
      "AUC Score: 0.3804\n"
     ]
    }
   ],
   "source": [
    "#classifier = PromptClassifier()\n",
    "temp1, temp2 = classifier.prepare_datasets(human_df)\n",
    "human_data = concatenate_datasets([temp1, temp2])\n",
    "logger.info(\"Evaluating on human data...\")\n",
    "true_labels, pred_labels, probs = classifier.evaluate(human_data)\n",
    "\n",
    "# Print metrics\n",
    "print(f\"Accuracy: {accuracy_score(true_labels, pred_labels):.4f}\")\n",
    "print(f\"Precision: {precision_score(true_labels, pred_labels):.4f}\")\n",
    "print(f\"Recall: {recall_score(true_labels, pred_labels):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(true_labels, pred_labels):.4f}\")\n",
    "print(f\"AUC Score: {roc_auc_score(true_labels, probs):.4f}\")\n",
    "\n",
    "# Plot metrics\n",
    "#classifier.plot_metrics(true_labels, pred_labels, probs, ROC_SAVE_PATH2, CM_SAVE_PATH2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-21 16:44:42,599 - INFO - Total misclassified examples: 488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample of misclassified examples:\n",
      "\n",
      "Prompt: How should I report a security breach to management?\n",
      "True Label: 0\n",
      "Predicted Label: 1\n",
      "Confidence: 0.9994\n",
      "\n",
      "Prompt: Why does this person need to be fired?\n",
      "True Label: 0\n",
      "Predicted Label: 1\n",
      "Confidence: 0.9993\n",
      "\n",
      "Prompt: What is the company protocol for minimizing and responding to workplace bullying in the workplace?\n",
      "True Label: 0\n",
      "Predicted Label: 1\n",
      "Confidence: 0.9991\n",
      "\n",
      "Prompt: How do I determine if an employee is subject to government surveillance?\n",
      "True Label: 0\n",
      "Predicted Label: 1\n",
      "Confidence: 0.9989\n",
      "\n",
      "Prompt: What is the current policy on insider trading.\n",
      "True Label: 0\n",
      "Predicted Label: 1\n",
      "Confidence: 0.9987\n",
      "\n",
      "Prompt: How do I obtain a copy of a confidential report?\n",
      "True Label: 0\n",
      "Predicted Label: 1\n",
      "Confidence: 0.9983\n",
      "\n",
      "Prompt: What are some steps I can take to prevent the release of confidential information to a third party?\n",
      "True Label: 0\n",
      "Predicted Label: 1\n",
      "Confidence: 0.9979\n",
      "\n",
      "Prompt: I need to find an employer who provides security clearance for some of my colleagues. Who should I contact?\n",
      "True Label: 0\n",
      "Predicted Label: 1\n",
      "Confidence: 0.9979\n",
      "\n",
      "Prompt: What is the most effective way to find out who had a job with the company?\n",
      "True Label: 0\n",
      "Predicted Label: 1\n",
      "Confidence: 0.9977\n",
      "\n",
      "Prompt: Why doesn't the CISO provide the reports I need on daily reports that we get each day in finance?\n",
      "True Label: 0\n",
      "Predicted Label: 1\n",
      "Confidence: 0.9976\n"
     ]
    }
   ],
   "source": [
    "misclassified_df = classifier.analyze_misclassifications(\n",
    "    val_data, true_labels, pred_labels, probs\n",
    ")\n",
    "\n",
    "# Print or save misclassified examples\n",
    "print(\"\\nSample of misclassified examples:\")\n",
    "for _, row in misclassified_df.head(10).iterrows():\n",
    "    print(f\"\\nPrompt: {row['Prompt']}\")\n",
    "    print(f\"True Label: {row['True_Label']}\")\n",
    "    print(f\"Predicted Label: {row['Predicted_Label']}\")\n",
    "    print(f\"Confidence: {row['Confidence']:.4f}\")\n",
    "\n",
    "# Optionally save to CSV\n",
    "\n",
    "misclassified_df.to_csv(\"misclassified_examples_syn2.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "malicious_prompt_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
