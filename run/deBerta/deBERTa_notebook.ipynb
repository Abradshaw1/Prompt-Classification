{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dcli/miniconda3/envs/malicious_prompt_env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from typing import Tuple, Dict, Any\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datasets import Dataset\n",
    "from sklearn.metrics import (\n",
    "    roc_curve, auc, confusion_matrix, accuracy_score,\n",
    "    precision_recall_fscore_support, precision_score,\n",
    "    recall_score, f1_score, roc_auc_score\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSequenceClassification,\n",
    "    Trainer, TrainingArguments, EarlyStoppingCallback\n",
    ")\n",
    "from peft import get_peft_model, LoraConfig, TaskType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-19 21:50:22,244 - INFO - Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Constants\n",
    "MODEL_NAME = \"microsoft/deberta-v3-small\"\n",
    "MAX_LENGTH = 256\n",
    "BATCH_SIZE = 8\n",
    "NUM_EPOCHS = 100\n",
    "WEIGHT_DECAY = 0.01\n",
    "MODEL_SAVE_PATH = \"./deberta_lora_classifier_full\"\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(\"DeBERTa Initial Runs\")\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logger.info(f\"Using device: {device}\")\n",
    "\n",
    "class PromptClassifier:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the classifier with model, tokenizer, and LoRA config.\"\"\"\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            MODEL_NAME, use_fast=False\n",
    "        )\n",
    "        self.base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            MODEL_NAME, num_labels=2\n",
    "        ).to(device)\n",
    "\n",
    "        lora_config = LoraConfig(\n",
    "            task_type=TaskType.SEQ_CLS,\n",
    "            r=8,\n",
    "            lora_alpha=16,\n",
    "            lora_dropout=0.1\n",
    "        )\n",
    "        self.model = get_peft_model(self.base_model, lora_config).to(device)\n",
    "        logger.info(\"DeBERTa LoRA model initialized.\")\n",
    "\n",
    "    def preprocess_data(self, examples: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Preprocess the input data using the tokenizer.\"\"\"\n",
    "        return self.tokenizer(\n",
    "            examples[\"Prompt\"],\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=MAX_LENGTH\n",
    "        ).to(device)\n",
    "\n",
    "    def prepare_datasets(self, df: pd.DataFrame) -> Tuple[Dataset, Dataset]:\n",
    "        \"\"\"Prepare training and validation datasets.\"\"\"\n",
    "        train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "            df[\"Prompt\"].tolist(),\n",
    "            df[\"Malicious (0/1)\"].tolist(),\n",
    "            test_size=0.2,\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "        train_data = Dataset.from_dict({\n",
    "            \"Prompt\": train_texts,\n",
    "            \"label\": train_labels\n",
    "        })\n",
    "        val_data = Dataset.from_dict({\n",
    "            \"Prompt\": val_texts,\n",
    "            \"label\": val_labels\n",
    "        })\n",
    "\n",
    "        train_data = train_data.map(self.preprocess_data, batched=True)\n",
    "        val_data = val_data.map(self.preprocess_data, batched=True)\n",
    "\n",
    "        return train_data, val_data\n",
    "\n",
    "    def compute_metrics(\n",
    "        self, eval_pred: Tuple[np.ndarray, np.ndarray]\n",
    "    ) -> Dict[str, float]:\n",
    "        \"\"\"Compute evaluation metrics.\"\"\"\n",
    "        logits, labels = eval_pred\n",
    "        predictions = torch.argmax(torch.tensor(logits), dim=-1)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "            labels, predictions, average='binary'\n",
    "        )\n",
    "        acc = accuracy_score(labels, predictions)\n",
    "        return {\n",
    "            \"accuracy\": acc,\n",
    "            \"f1\": f1,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall\n",
    "        }\n",
    "\n",
    "    def train(self, train_data: Dataset, val_data: Dataset) -> None:\n",
    "        \"\"\"Train the model.\"\"\"\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=\"./results\",\n",
    "            eval_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            per_device_train_batch_size=BATCH_SIZE,\n",
    "            per_device_eval_batch_size=BATCH_SIZE,\n",
    "            num_train_epochs=NUM_EPOCHS,\n",
    "            weight_decay=WEIGHT_DECAY,\n",
    "            logging_dir=\"./deberta_logs\",\n",
    "            logging_strategy=\"steps\",\n",
    "            logging_steps=50,\n",
    "            save_total_limit=3,\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"eval_loss\",\n",
    "            greater_is_better=False,\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            report_to=\"none\"\n",
    "        )\n",
    "\n",
    "        early_stopping_callback = EarlyStoppingCallback(\n",
    "            early_stopping_patience=3,\n",
    "            early_stopping_threshold=0.0\n",
    "        )\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_data,\n",
    "            eval_dataset=val_data,\n",
    "            compute_metrics=self.compute_metrics,\n",
    "            callbacks=[early_stopping_callback]\n",
    "        )\n",
    "\n",
    "        trainer.train()\n",
    "        self.save_model()\n",
    "\n",
    "    def save_model(self) -> None:\n",
    "        \"\"\"Save the trained model and tokenizer.\"\"\"\n",
    "        self.model.save_pretrained(MODEL_SAVE_PATH)\n",
    "        self.tokenizer.save_pretrained(MODEL_SAVE_PATH)\n",
    "        logger.info(f\"Model saved to {MODEL_SAVE_PATH}\")\n",
    "\n",
    "    def evaluate(\n",
    "        self, val_data: Dataset\n",
    "    ) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "        \"\"\"Evaluate the model and return predictions.\"\"\"\n",
    "        test_trainer = Trainer(self.model)\n",
    "        predictions = test_trainer.predict(val_data)\n",
    "\n",
    "        logits = torch.tensor(predictions.predictions)\n",
    "        probs = torch.nn.functional.softmax(logits, dim=-1).numpy()\n",
    "        pred_labels = np.argmax(probs, axis=-1)\n",
    "\n",
    "        return predictions.label_ids, pred_labels, probs[:, 1]\n",
    "\n",
    "    def plot_metrics(\n",
    "        self, true_labels: np.ndarray, pred_labels: np.ndarray,\n",
    "        probs: np.ndarray\n",
    "    ) -> None:\n",
    "        \"\"\"Plot ROC curve and confusion matrix.\"\"\"\n",
    "        # ROC Curve\n",
    "        fpr, tpr, _ = roc_curve(true_labels, probs)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(\n",
    "            fpr, tpr, color=\"blue\", lw=2,\n",
    "            label=f\"ROC Curve (AUC = {roc_auc:.2f})\"\n",
    "        )\n",
    "        plt.plot([0, 1], [0, 1], color=\"gray\", linestyle=\"--\")\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel(\"False Positive Rate\")\n",
    "        plt.ylabel(\"True Positive Rate\")\n",
    "        plt.title(\"Receiver Operating Characteristic (ROC) Curve\")\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.savefig(\n",
    "            \"roc_curve_lora_full.png\",\n",
    "            dpi=300,\n",
    "            bbox_inches=\"tight\"\n",
    "        )\n",
    "        plt.close()\n",
    "\n",
    "        # Confusion Matrix\n",
    "        cm = confusion_matrix(true_labels, pred_labels)\n",
    "        plt.figure(figsize=(6, 5))\n",
    "        sns.heatmap(\n",
    "            cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=[\"0\", \"1\"], yticklabels=[\"0\", \"1\"]\n",
    "        )\n",
    "        plt.xlabel(\"Predicted Label\")\n",
    "        plt.ylabel(\"True Label\")\n",
    "        plt.title(\"Confusion Matrix\")\n",
    "        plt.savefig(\n",
    "            \"confusion_matrix_lora_full.png\",\n",
    "            dpi=300,\n",
    "            bbox_inches=\"tight\"\n",
    "        )\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-03-19 21:50:26,802 - INFO - DeBERTa LoRA model initialized.\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 61457/61457 [00:04<00:00, 13778.57 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15365/15365 [00:01<00:00, 13889.37 examples/s]\n",
      "2025-03-19 21:50:36,560 - INFO - Training: 30768 malicious prompts\n",
      "2025-03-19 21:50:36,561 - INFO - Training: 30689 benign prompts\n",
      "2025-03-19 21:50:36,561 - INFO - Validation: 7679 malicious prompts\n",
      "2025-03-19 21:50:36,561 - INFO - Validation: 7686 benign prompts\n",
      "2025-03-19 21:50:36,562 - INFO - Starting fine-tuning...\n",
      "/home/dcli/miniconda3/envs/malicious_prompt_env/lib/python3.9/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='76830' max='768300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 76830/768300 37:24 < 5:36:41, 34.23 it/s, Epoch 10/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.172000</td>\n",
       "      <td>0.164165</td>\n",
       "      <td>0.952099</td>\n",
       "      <td>0.951515</td>\n",
       "      <td>0.962805</td>\n",
       "      <td>0.940487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.155200</td>\n",
       "      <td>0.157677</td>\n",
       "      <td>0.957631</td>\n",
       "      <td>0.958474</td>\n",
       "      <td>0.939360</td>\n",
       "      <td>0.978383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.162000</td>\n",
       "      <td>0.138905</td>\n",
       "      <td>0.960950</td>\n",
       "      <td>0.960795</td>\n",
       "      <td>0.964197</td>\n",
       "      <td>0.957416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.138300</td>\n",
       "      <td>0.135525</td>\n",
       "      <td>0.963619</td>\n",
       "      <td>0.963528</td>\n",
       "      <td>0.965481</td>\n",
       "      <td>0.961584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.114800</td>\n",
       "      <td>0.150929</td>\n",
       "      <td>0.964335</td>\n",
       "      <td>0.964503</td>\n",
       "      <td>0.959531</td>\n",
       "      <td>0.969527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.155700</td>\n",
       "      <td>0.137997</td>\n",
       "      <td>0.964139</td>\n",
       "      <td>0.963810</td>\n",
       "      <td>0.972303</td>\n",
       "      <td>0.955463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.155600</td>\n",
       "      <td>0.122361</td>\n",
       "      <td>0.966612</td>\n",
       "      <td>0.966807</td>\n",
       "      <td>0.960777</td>\n",
       "      <td>0.972913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.127500</td>\n",
       "      <td>0.123935</td>\n",
       "      <td>0.967459</td>\n",
       "      <td>0.967528</td>\n",
       "      <td>0.965021</td>\n",
       "      <td>0.970048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.099000</td>\n",
       "      <td>0.134544</td>\n",
       "      <td>0.966287</td>\n",
       "      <td>0.966015</td>\n",
       "      <td>0.973423</td>\n",
       "      <td>0.958719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.079800</td>\n",
       "      <td>0.131984</td>\n",
       "      <td>0.966938</td>\n",
       "      <td>0.967124</td>\n",
       "      <td>0.961276</td>\n",
       "      <td>0.973043</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-19 22:28:01,746 - INFO - Model saved to ./deberta_lora_classifier_full\n",
      "2025-03-19 22:28:01,747 - INFO - Evaluating...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9666\n",
      "Precision: 0.9608\n",
      "Recall: 0.9729\n",
      "F1 Score: 0.9668\n",
      "AUC Score: 0.9951\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "df = pd.read_csv(\n",
    "    \"./data/FINAL_validated_prompts_with_similarity_deBERTa.csv\"\n",
    ")\n",
    "df[\"Malicious (0/1)\"] = df[\"Malicious (0/1)\"].astype(int)\n",
    "\"\"\"\n",
    "# Separate malicious and benign prompts\n",
    "malicious_prompts = df[df[\"Malicious (0/1)\"] == 1]\n",
    "benign_prompts = df[df[\"Malicious (0/1)\"] == 0]\n",
    "\n",
    "# Randomly sample 3000 prompts from each class\n",
    "malicious_sample = malicious_prompts.sample(n=3000, random_state=42)\n",
    "benign_sample = benign_prompts.sample(n=3000, random_state=42)\n",
    "\n",
    "# Combine the samples\n",
    "balanced_df = pd.concat([malicious_sample, benign_sample])\n",
    "# Shuffle the combined dataset\n",
    "balanced_df = balanced_df.sample(frac=1, random_state=42) \\\n",
    "        .reset_index(drop=True)\n",
    "\n",
    "logger.info(f\"Created balanced dataset with {len(balanced_df)} rows\")\n",
    "logger.info(f\"Number of malicious prompts: {sum(balanced_df['Malicious (0/1)'] == 1)}\")\n",
    "logger.info(f\"Number of benign prompts: {sum(balanced_df['Malicious (0/1)'] == 0)}\")\n",
    "\"\"\"\n",
    "# Initialize and train classifier\n",
    "classifier = PromptClassifier()\n",
    "train_data, val_data = classifier.prepare_datasets(df)\n",
    "\n",
    "train_total = len(train_data['label'])\n",
    "val_total = len(val_data['label'])\n",
    "train_1_total = sum(train_data['label'])\n",
    "val_1_total = sum(val_data['label'])\n",
    "logger.info(f\"Training: {train_1_total} malicious prompts\")\n",
    "logger.info(f\"Training: {train_total-train_1_total} benign prompts\")\n",
    "logger.info(f\"Validation: {val_1_total} malicious prompts\")\n",
    "logger.info(f\"Validation: {val_total-val_1_total} benign prompts\")\n",
    "    \n",
    "logger.info(\"Starting fine-tuning...\")\n",
    "classifier.train(train_data, val_data)\n",
    "\n",
    "# Evaluate and plot results\n",
    "logger.info(\"Evaluating...\")\n",
    "true_labels, pred_labels, probs = classifier.evaluate(val_data)\n",
    "\n",
    "# Print metrics\n",
    "print(f\"Accuracy: {accuracy_score(true_labels, pred_labels):.4f}\")\n",
    "print(f\"Precision: {precision_score(true_labels, pred_labels):.4f}\")\n",
    "print(f\"Recall: {recall_score(true_labels, pred_labels):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(true_labels, pred_labels):.4f}\")\n",
    "print(f\"AUC Score: {roc_auc_score(true_labels, probs):.4f}\")\n",
    "\n",
    "# Plot metrics\n",
    "classifier.plot_metrics(true_labels, pred_labels, probs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "malicious_prompt_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
