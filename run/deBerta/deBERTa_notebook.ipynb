{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dcli/miniconda3/envs/malicious_prompt_env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from typing import Tuple, Dict, Any\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datasets import Dataset\n",
    "from sklearn.metrics import (\n",
    "    roc_curve, auc, confusion_matrix, accuracy_score,\n",
    "    precision_recall_fscore_support, precision_score,\n",
    "    recall_score, f1_score, roc_auc_score\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSequenceClassification,\n",
    "    Trainer, TrainingArguments, EarlyStoppingCallback\n",
    ")\n",
    "from peft import get_peft_model, LoraConfig, TaskType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-19 21:23:27,180 - INFO - Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Constants\n",
    "MODEL_NAME = \"microsoft/deberta-v3-small\"\n",
    "MAX_LENGTH = 256\n",
    "BATCH_SIZE = 8\n",
    "NUM_EPOCHS = 100\n",
    "WEIGHT_DECAY = 0.01\n",
    "MODEL_SAVE_PATH = \"./deberta_lora_classifier2\"\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(\"DeBERTa Initial Runs\")\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logger.info(f\"Using device: {device}\")\n",
    "\n",
    "class PromptClassifier:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the classifier with model, tokenizer, and LoRA config.\"\"\"\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            MODEL_NAME, use_fast=False\n",
    "        )\n",
    "        self.base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            MODEL_NAME, num_labels=2\n",
    "        ).to(device)\n",
    "\n",
    "        lora_config = LoraConfig(\n",
    "            task_type=TaskType.SEQ_CLS,\n",
    "            r=8,\n",
    "            lora_alpha=16,\n",
    "            lora_dropout=0.1\n",
    "        )\n",
    "        self.model = get_peft_model(self.base_model, lora_config).to(device)\n",
    "        logger.info(\"DeBERTa LoRA model initialized.\")\n",
    "\n",
    "    def preprocess_data(self, examples: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Preprocess the input data using the tokenizer.\"\"\"\n",
    "        return self.tokenizer(\n",
    "            examples[\"Prompt\"],\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=MAX_LENGTH\n",
    "        ).to(device)\n",
    "\n",
    "    def prepare_datasets(self, df: pd.DataFrame) -> Tuple[Dataset, Dataset]:\n",
    "        \"\"\"Prepare training and validation datasets.\"\"\"\n",
    "        train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "            df[\"Prompt\"].tolist(),\n",
    "            df[\"Malicious (0/1)\"].tolist(),\n",
    "            test_size=0.2,\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "        train_data = Dataset.from_dict({\n",
    "            \"Prompt\": train_texts,\n",
    "            \"label\": train_labels\n",
    "        })\n",
    "        val_data = Dataset.from_dict({\n",
    "            \"Prompt\": val_texts,\n",
    "            \"label\": val_labels\n",
    "        })\n",
    "\n",
    "        train_data = train_data.map(self.preprocess_data, batched=True)\n",
    "        val_data = val_data.map(self.preprocess_data, batched=True)\n",
    "\n",
    "        return train_data, val_data\n",
    "\n",
    "    def compute_metrics(\n",
    "        self, eval_pred: Tuple[np.ndarray, np.ndarray]\n",
    "    ) -> Dict[str, float]:\n",
    "        \"\"\"Compute evaluation metrics.\"\"\"\n",
    "        logits, labels = eval_pred\n",
    "        predictions = torch.argmax(torch.tensor(logits), dim=-1)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "            labels, predictions, average='binary'\n",
    "        )\n",
    "        acc = accuracy_score(labels, predictions)\n",
    "        return {\n",
    "            \"accuracy\": acc,\n",
    "            \"f1\": f1,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall\n",
    "        }\n",
    "\n",
    "    def train(self, train_data: Dataset, val_data: Dataset) -> None:\n",
    "        \"\"\"Train the model.\"\"\"\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=\"./results\",\n",
    "            eval_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            per_device_train_batch_size=BATCH_SIZE,\n",
    "            per_device_eval_batch_size=BATCH_SIZE,\n",
    "            num_train_epochs=NUM_EPOCHS,\n",
    "            weight_decay=WEIGHT_DECAY,\n",
    "            logging_dir=\"./deberta_logs\",\n",
    "            logging_strategy=\"steps\",\n",
    "            logging_steps=50,\n",
    "            save_total_limit=3,\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"eval_loss\",\n",
    "            greater_is_better=False,\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            report_to=\"none\"\n",
    "        )\n",
    "\n",
    "        early_stopping_callback = EarlyStoppingCallback(\n",
    "            early_stopping_patience=3,\n",
    "            early_stopping_threshold=0.0\n",
    "        )\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_data,\n",
    "            eval_dataset=val_data,\n",
    "            compute_metrics=self.compute_metrics,\n",
    "            callbacks=[early_stopping_callback]\n",
    "        )\n",
    "\n",
    "        trainer.train()\n",
    "        self.save_model()\n",
    "\n",
    "    def save_model(self) -> None:\n",
    "        \"\"\"Save the trained model and tokenizer.\"\"\"\n",
    "        self.model.save_pretrained(MODEL_SAVE_PATH)\n",
    "        self.tokenizer.save_pretrained(MODEL_SAVE_PATH)\n",
    "        logger.info(f\"Model saved to {MODEL_SAVE_PATH}\")\n",
    "\n",
    "    def evaluate(\n",
    "        self, val_data: Dataset\n",
    "    ) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "        \"\"\"Evaluate the model and return predictions.\"\"\"\n",
    "        test_trainer = Trainer(self.model)\n",
    "        predictions = test_trainer.predict(val_data)\n",
    "\n",
    "        logits = torch.tensor(predictions.predictions)\n",
    "        probs = torch.nn.functional.softmax(logits, dim=-1).numpy()\n",
    "        pred_labels = np.argmax(probs, axis=-1)\n",
    "\n",
    "        return predictions.label_ids, pred_labels, probs[:, 1]\n",
    "\n",
    "    def plot_metrics(\n",
    "        self, true_labels: np.ndarray, pred_labels: np.ndarray,\n",
    "        probs: np.ndarray\n",
    "    ) -> None:\n",
    "        \"\"\"Plot ROC curve and confusion matrix.\"\"\"\n",
    "        # ROC Curve\n",
    "        fpr, tpr, _ = roc_curve(true_labels, probs)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(\n",
    "            fpr, tpr, color=\"blue\", lw=2,\n",
    "            label=f\"ROC Curve (AUC = {roc_auc:.2f})\"\n",
    "        )\n",
    "        plt.plot([0, 1], [0, 1], color=\"gray\", linestyle=\"--\")\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel(\"False Positive Rate\")\n",
    "        plt.ylabel(\"True Positive Rate\")\n",
    "        plt.title(\"Receiver Operating Characteristic (ROC) Curve\")\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.savefig(\n",
    "            \"roc_curve_lora.png\",\n",
    "            dpi=300,\n",
    "            bbox_inches=\"tight\"\n",
    "        )\n",
    "        plt.close()\n",
    "\n",
    "        # Confusion Matrix\n",
    "        cm = confusion_matrix(true_labels, pred_labels)\n",
    "        plt.figure(figsize=(6, 5))\n",
    "        sns.heatmap(\n",
    "            cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=[\"0\", \"1\"], yticklabels=[\"0\", \"1\"]\n",
    "        )\n",
    "        plt.xlabel(\"Predicted Label\")\n",
    "        plt.ylabel(\"True Label\")\n",
    "        plt.title(\"Confusion Matrix\")\n",
    "        plt.savefig(\n",
    "            \"confusion_matrix_lora.png\",\n",
    "            dpi=300,\n",
    "            bbox_inches=\"tight\"\n",
    "        )\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-19 21:26:41,088 - INFO - Created balanced dataset with 6000 rows\n",
      "2025-03-19 21:26:41,088 - INFO - Number of malicious prompts: 3000\n",
      "2025-03-19 21:26:41,089 - INFO - Number of benign prompts: 3000\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-03-19 21:26:41,523 - INFO - DeBERTa LoRA model initialized.\n",
      "Map: 100%|██████████| 4800/4800 [00:00<00:00, 9926.85 examples/s] \n",
      "Map: 100%|██████████| 1200/1200 [00:00<00:00, 12304.25 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'bool' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 29\u001b[0m\n\u001b[1;32m     27\u001b[0m train_data, val_data \u001b[38;5;241m=\u001b[39m classifier\u001b[38;5;241m.\u001b[39mprepare_datasets(balanced_df)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28msum\u001b[39m(train_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m10\u001b[39m]))\n\u001b[0;32m---> 29\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msum\u001b[39m(train_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;250m \u001b[39m\u001b[38;5;241m==\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m malicious prompts\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     30\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msum\u001b[39m(train_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;250m \u001b[39m\u001b[38;5;241m==\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m benign prompts\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     31\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msum\u001b[39m(val_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;250m \u001b[39m\u001b[38;5;241m==\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m malicious prompts\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'bool' object is not iterable"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "df = pd.read_csv(\n",
    "    \"./data/FINAL_validated_prompts_with_similarity_deBERTa.csv\"\n",
    ")\n",
    "df[\"Malicious (0/1)\"] = df[\"Malicious (0/1)\"].astype(int)\n",
    "\n",
    "# Separate malicious and benign prompts\n",
    "malicious_prompts = df[df[\"Malicious (0/1)\"] == 1]\n",
    "benign_prompts = df[df[\"Malicious (0/1)\"] == 0]\n",
    "\n",
    "# Randomly sample 3000 prompts from each class\n",
    "malicious_sample = malicious_prompts.sample(n=3000, random_state=42)\n",
    "benign_sample = benign_prompts.sample(n=3000, random_state=42)\n",
    "\n",
    "# Combine the samples\n",
    "balanced_df = pd.concat([malicious_sample, benign_sample])\n",
    "# Shuffle the combined dataset\n",
    "balanced_df = balanced_df.sample(frac=1, random_state=42) \\\n",
    "        .reset_index(drop=True)\n",
    "\n",
    "logger.info(f\"Created balanced dataset with {len(balanced_df)} rows\")\n",
    "logger.info(f\"Number of malicious prompts: {sum(balanced_df['Malicious (0/1)'] == 1)}\")\n",
    "logger.info(f\"Number of benign prompts: {sum(balanced_df['Malicious (0/1)'] == 0)}\")\n",
    "\n",
    "# Initialize and train classifier\n",
    "classifier = PromptClassifier()\n",
    "train_data, val_data = classifier.prepare_datasets(balanced_df)\n",
    "\n",
    "train_total = len(train_data['label'])\n",
    "val_total = len(val_data['label'])\n",
    "train_1_total = sum(train_data['label'])\n",
    "val_1_total = sum(val_data['label'])\n",
    "logger.info(f\"Training: {train_1_total} malicious prompts\")\n",
    "logger.info(f\"Training: {train_total-train_1_total} benign prompts\")\n",
    "logger.info(f\"Validation: {val_1_total} malicious prompts\")\n",
    "logger.info(f\"Validation: {val_total-val_1_total} benign prompts\")\n",
    "    \n",
    "logger.info(\"Starting fine-tuning...\")\n",
    "classifier.train(train_data, val_data)\n",
    "\n",
    "# Evaluate and plot results\n",
    "logger.info(\"Evaluating...\")\n",
    "true_labels, pred_labels, probs = classifier.evaluate(val_data)\n",
    "\n",
    "# Print metrics\n",
    "print(f\"Accuracy: {accuracy_score(true_labels, pred_labels):.4f}\")\n",
    "print(f\"Precision: {precision_score(true_labels, pred_labels):.4f}\")\n",
    "print(f\"Recall: {recall_score(true_labels, pred_labels):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(true_labels, pred_labels):.4f}\")\n",
    "print(f\"AUC Score: {roc_auc_score(true_labels, probs):.4f}\")\n",
    "\n",
    "# Plot metrics\n",
    "classifier.plot_metrics(true_labels, pred_labels, probs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "malicious_prompt_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
